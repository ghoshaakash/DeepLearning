\chapterimage{jigar-panchal-TVyPnkS5k5w-unsplash.jpg} % Chapter heading image
\chapterspaceabove{6.25cm} % Whitespace from the top of the page to the chapter title on chapter pages
\chapterspacebelow{8.5cm} % Amount of vertical whitespace from the top margin to the start of the text on chapter pages

%------------------------------------------------

\chapter{Attention Models}

\section{Intuition}
We formally define the problem to be one where given a sequence $\{x_i\}_{i\in\mathbb N}$ we want to predict another sequence $\{y_i\}_{i\in\mathbb N}$. We have seen that previously RNNs were used. Then we switched to LSTMs(and it's variants like GRNs) and then ended with encoder decoder models. The main problem is the the latent representation of the input sequence is not built to carry a large amount of information. So the beginning of the sequence which is to be encoded gets lost. We look at a possible solution: Along with the latent  representation $z$, previous output $y_{t-1}$ and previous hidden state $h_{t-1}$, we also send $\tilde x$ or the average information to the RNN. 