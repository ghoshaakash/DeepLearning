\chapterimage{jigar-panchal-TVyPnkS5k5w-unsplash.jpg} % Chapter heading image
\chapterspaceabove{6.25cm} % Whitespace from the top of the page to the chapter title on chapter pages
\chapterspacebelow{8.5cm} % Amount of vertical whitespace from the top margin to the start of the text on chapter pages

%------------------------------------------------

\chapter{Earlier Works}
\section{Introduction}
Although the current state of the art uses transformers(or at least they did at the time of writing), language models existed way before that. Here I will look and discuss some of them, mostly based on Andrej Karpathy's videos on YouTube.


\section{Bigram models}
In a bigram model, the assumption is that the next word/token depends entirely on the current word or token(which is a very bad assumption, but good enough for a naive first try). Formally speaking, the training data consists of pairs of words $w_1,w_2$ which occur consecutively. We construct a matrix $M$ where $M[i][j]$ is the probability that if $w_1$ occurs, the next probable word is $w_2$. Now, once the word $w_i$ occurs, the model picks the next word from the multinomial distribution generated by the row of $M[i]$. It may seem obvious that  $M[i][j]$ is simply the normalized count of the occurrence of $w_i$ after $w_j$(with some modifications if we smoothen stuff).   


Krapathy "trains" a model to do that on a letter level, but I believe it will work better if the tokens are on the level of words, simply because words contain more positional information that letters.  But even after that, it doesn't work very well as the model simply doesn't care about history of more than 1 word. We can of course generalize it to an $n$-gram model, but the space complexity is $\mathcal O(|V|^n)$ where $|V|$ is the size of the vocabulary. 


\section{MLP models with word embeddings}
We will be mostly inspired by the works of \cite{bengio2000neural}. The main idea is to bypass the curse of dimensionality by generating a latent low-dimensional representation, similar to word2vec and skip-gram. 