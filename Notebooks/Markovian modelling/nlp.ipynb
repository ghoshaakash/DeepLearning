{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ghosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"corpus.txt\"\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(file_path, \"r\") as file:\n",
    "    txt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=word_tokenize(txt.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# Initialize the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word and create a list of stemmed words\n",
    "txt = [stemmer.stem(word) for word in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', 'of', '.', 'and', 'a', 'to', 'their', 'is', 'that', 'with', 'it', 'dog', 'in', \"'s\", 'cat', 'as', 'countri', 'pet', 'our', 'from', 'they', 'human', 'own', 'an', 'have', 'divers', 'nation', 'live', 'uniqu', 'companion', 'on', 'are', 'for', 'cultur', 'india', 'companionship', 'bond', 'each', 'love', 'often', 'them', 'america', 'essay', ':', 'joy', 'way', 'us', 'play', 'world', 'or', 'testament', 'provid', 'independ', 'by', 'natur', 'groom', 'emot', 'physic', 'offer', 'sens', 'time', 'bring', 'eleg', 'becom', 'between', 'shape', 'friend', 'loyalti', 'at', 'not', 'also', 'movement', 'captiv', 'care', 'gentl', 'can', 'stress', 'day', 'beauti', 'we', 'more', 'make', 'form', 'like', 'histori', 'peopl', 'tradit', 'landscap', 'uniti', 'reflect', 'experi', 'cherish', 'connect', '``', \"''\", 'unwav', 'onli', 'but', 'activ', 'lifestyl', 'owner', 'walk', 'air', 'mysteri', 'find', 'content', 'beyond', 'benefit', 'lower', 'routin', 'respons', 'be', 'well-b', 'conclus', 'qualiti', 'uncondit', 'these', 'fulfil', 'place', 'enchant', 'grace', 'even', 'most', 'everi', 'while', 'countless', 'heart', 'celebr', 'whether', 'contribut', 'ident', 'tapestri', 'flag', 'state', 'struggl', 'american', 'vibrant', '1', 'wag', 'tail', 'presenc', 'member', 'famili', 'anim', 'transcend', 'come', 'set', 'characterist', 'known', 'man', 'best', 'boundless', 'devot', 'encourag', 'accompani', 'adventur', 'other', 'hand', 'leav', 'enigmat', 'meticul', 'pur', 'sooth', 'away', 'ha', 'mental', 'studi', 'shown', 'reduc', 'level', 'blood', 'pressur', 'those', 'feed', 'ownership', 'attent', 'understand', 'speci', 'immeasur', 'distinct', 'enrich', 'profound', 'teach', 'remind', 'share', 'brighter', '2', 'carri', 'behavior', 'fascin', 'â€', '“', 'innat', 'one', 'endear', 'ritual', 'ensur', 'thi', 'maintain', 'melodi', 'varieti', 'yowl', 'differ', 'allow', 'deep', 'choos', 'moment', 'out', 'your', 'lap', 'against', 'trust', 'household', 'into', '3', 'unbreak', 'refer', 'embodi', 'remark', 'individu', 'unit', 'than', 'character', 'impact', 'home', 'rich', 'stori', 'explor', 'languag', 'peak', 'reson', 'citizen', 'mosaic', 'flavor', 'spici', 'culinari', 'rang', 'dream', 'opportun', 'across', 'north', '50', 'woven', 'civil', 'equal', 'symbol', 'freedom', 'music', 'icon', 'stand', 'hope', 'ancient', 'hue', 'unparallel', 'word', 'variou', 'size', 'exhibit', 'enthusiasm', 'excit', 'mere', 'sight', 'heartwarm', 'display', 'eagerli', 'run', 'outdoor', 'pierc', 'gaze', 'intrigu', 'seemingli', 'adept', 'cozi', 'spot', 'curl', 'up', 'bask', 'sunlight', 'themselv', 'calm', 'effect', 'numer', 'furri', 'allevi', 'feel', 'loneli', 'isol', 'particularli', 'alon', 'structur', 'purpos', 'howev', 'should', 'taken', 'lightli', 'depend', 'commit', 'proper', 'nutrit', 'regular', 'veterinari', 'visit', 'need', 'essenti', 'compon', 'requir', 'crucial', 'nurtur', 'environ', 'person', 'valuabl', 'lesson', 'empathi', 'loyal', 'allur', 'centuri', 'creatur', 'apart', 'both', 'agil', 'hunt', 'prowess', 'lith', 'bodi', 'sharp', 'reflex', 'enabl', 'navig', 'precari', 'height', 'watch', 'stalk', 'prey', 'toy', 'stray', 'sunbeam', 'reveal', 'instinct', 'action', 'which', 'pounc', 'laughter', 'anyon', 'fortun', 'enough', 'wit', 'clean', 'fur', 'use', 'tongu', 'inch', 'immacul', 'self-car', 'appear', 'act', 'relax', 'eas', 'commun', 'through', 'sound', 'soft', 'meow', 'loud', 'vocal', 'mean', 'deciph', 'felin', \"'\", 'intent', 'purr', 'signifi', 'sudden', 'might', 'indic', 'plea', 'declar', 'territori', 'counterpart', 'affect', 'seek', 'cuddl', 'warmth', 'nestl', 'touch', 'paw', 'belov', 'addit', 'weav', 'last', 'impress', 'steadfast', 'energi', 'made', 'comfort', 'relationship', 'date', 'back', 'thousand', 'year', 'built', 'foundat', 'mutual', 'possess', 'abil', 'solac', 'distress', 'dure', 'enthusiast', 'door', 'bark', 'greet', 'you', 'uncanni', 'brighten', 'dullest', 'petit', 'chihuahua', 'majest', 'great', 'dane', 'breed', 'trait', 'match', 'partner', 'inclin', 'integr', 'part', 'just', ';', 'confid', 'protector', 'playmat', 'secur', 'extend', 'far', 'improv', 'spend', 'anxieti', 'daili', 'promot', 'healthi', 'exist', 'posit', 'invalu', 'simpl', 'pleasur', 'friendship', 'transform', 'power', 'open', 'four-leg', 'geograph', 'entiti', 'hold', 'within', 'wait', 'big', 'small', 'boast', 'custom', 'snowi', 'switzerland', 'bustl', 'street', 'japan', 'glimps', 'cover', 'planet', 'wave', 'proudli', 'wind', 'spirit', 'anthem', 'ring', 'fill', 'region', 'cuisin', 'hearti', 'dish', 'germani', 'who', 'call', 'govern', 'vital', 'role', 'destini', 'decis', 'educ', 'healthcar', 'infrastructur', 'economi', 'law', 'regul', 'help', 'order', 'resid', 'same', 'heritag', 'add', 'global', 'signific', 'draw', 'tourist', 'scholar', 'alik', 'wonder', 'land', 'stretch', 'contin', 'melt', 'pot', 'popul', 'skyscrap', 'new', 'york', 'citi', 'seren', 'beach', 'california', 'vari', 'thrive', 'triumph', 'fight', 'british', 'rule', 'cornerston', 'legaci', 'abraham', 'lincoln', 'right', 'exemplifi', 'ongo', 'justic', '13', 'stripe', 'star', 'under', 'singl', 'life', 'pursuit', 'drawn', 'million', 'shore', 'search', 'better', 'includ', 'cinema', 'sport', 'influenc', 'span', 'globe', 'thanksgiv', 'valu', 'landmark', 'statu', 'liberti', 'beacon', 'subcontin', 'stimul', 'soul', 'snow-cap', 'himalaya', 'lush', 'backwat', 'kerala', 'geographi', 'grandeur', 'thread', 'dynasti', 'coloni', 'creat', 'hallmark', 'over', '2,000', 'ethnic', 'group', '1,600', 'spoken', 'festiv', 'diwali', 'holi', 'eid', 'togeth', 'rhythmic', 'beat', 'classic', 'danc', 'audienc', 'worldwid', 'curri', 'south', 'aromat', 'biryani', 'spiritu', 'philosophi', 'left', 'indel', 'mark', 'mahatma', 'gandhi', 'text', 'veda', 'wisdom', 'taj', 'mahal', 'architectur', 'brillianc', 'saffron', 'white', 'green', 'indian', 'puriti', 'futur']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the frequency of each word using Counter\n",
    "word_frequency = Counter(txt)\n",
    "\n",
    "# Get unique words ordered by decreasing frequency\n",
    "vocab = sorted(word_frequency.keys(), key=lambda word: word_frequency[word], reverse=True)\n",
    "\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=np.zeros((741,741))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map={}\n",
    "j=0\n",
    "for i in vocab:\n",
    "    hash_map[i]=j\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while(i<len(txt)-8):\n",
    "    iIndex=hash_map[txt[i]]\n",
    "    j=0\n",
    "    while(j<7):\n",
    "        jIndex=hash_map[txt[i+j]]\n",
    "        M[iIndex][jIndex]=M[iIndex][jIndex]+1\n",
    "        M[jIndex][iIndex]=M[iIndex][jIndex]\n",
    "        j=j+1\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the matrix M\n",
    "U, S, Vt = np.linalg.svd(M)\n",
    "\n",
    "# Choose the first n singular values and vectors for approximation\n",
    "n = 20\n",
    "U_n = U[:, :n]\n",
    "S_n = np.diag(S[:n])\n",
    "Vt_n = Vt[:n, :]\n",
    "\n",
    "# Reconstruct the n-th order SVD approximation\n",
    "M_n = U_n @ S_n @ Vt_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 15 37 39 \n"
     ]
    }
   ],
   "source": [
    "print(str(hash_map[\"dog\"])+\" \"+str(hash_map[\"cat\"])+\" \"+str(hash_map[\"bond\"])+\" \"+str(hash_map[\"love\"])+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[12,15,37,39]\n",
    "M_sub=np.zeros((4,4))\n",
    "i=0\n",
    "while(i<4):\n",
    "    j=0\n",
    "    while(j<4):\n",
    "        M_sub[i][j]=M_n[arr[i]][arr[j]]\n",
    "        j=j+1\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(matrix):\n",
    "    min_val = np.min(matrix)\n",
    "    max_val = np.max(matrix)\n",
    "    scaled_matrix = (matrix - min_val) / (max_val - min_val)\n",
    "    return scaled_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.53676558e-01, 2.55962921e-01, 9.11303126e-02],\n",
       "       [1.53676558e-01, 7.91483866e-01, 9.85905940e-02, 1.14938869e-16],\n",
       "       [2.55962921e-01, 9.85905940e-02, 1.84269464e-01, 6.73188278e-02],\n",
       "       [9.11303126e-02, 0.00000000e+00, 6.73188278e-02, 1.65745347e-01]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaling(M_sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
